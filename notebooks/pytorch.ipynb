{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "orig_nbformat": 4,
    "language_info": {
      "name": "python",
      "version": "3.8.10",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.10 64-bit ('brax': conda)"
    },
    "interpreter": {
      "hash": "1812c2cdf0067f1c111fe8b907e8717aab013f923026de8fa0a048a0a07a7c66"
    },
    "colab": {
      "name": "gym_pytorch.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQNZHOfQlor9"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/google/brax/blob/main/notebooks/pytorch.ipynb\" target=\"_blank\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Colab\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqqSTrs_lor_"
      },
      "source": [
        "# Using Brax with PyTorch\n",
        "\n",
        "Brax integrates seamlessly with [PyTorch](https://pytorch.org/) via a [Gym interface](https://github.com/openai/gym) without sacrificing any performance.  In this notebook we demonstrate how to operate Brax by passing in PyTorch arrays.\n",
        "\n",
        "Author: @lebrice (Fabrice Normandin)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z4sNbJ4smdIh"
      },
      "source": [
        "#@title Set up Colab dependencies\n",
        "#@markdown ### ⚠️ PLEASE NOTE:\n",
        "#@markdown Brax and PyTorch can share GPUs but not TPUs.  To access a GPU runtime, select Runtime > Change Runtime Type and set 'Hardware Accelerator' to `GPU`.\n",
        "#@markdown\n",
        "#@markdown Using TPU is OK too, but then PyTorch should run on CPU.\n",
        "\n",
        "import os\n",
        "\n",
        "try:\n",
        "  import brax\n",
        "except ImportError:\n",
        "  from IPython.display import clear_output \n",
        "  !pip install git+https://github.com/google/brax.git@main\n",
        "  clear_output()\n",
        "\n",
        "if 'COLAB_TPU_ADDR' in os.environ:\n",
        "  from jax.tools import colab_tpu\n",
        "  colab_tpu.setup_tpu()"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gKby-SR2losA",
        "outputId": "65305ce4-6e15-4146-b424-6e987a37e558"
      },
      "source": [
        "import time\n",
        "import torch\n",
        "from functools import partial\n",
        "\n",
        "import gym\n",
        "import tqdm\n",
        "import numpy as np\n",
        "\n",
        "from brax.envs.to_torch import JaxToTorchWrapper\n",
        "from brax.envs import _envs, create_gym_env\n",
        "\n",
        "# Registering the Brax envs in Gym (so we can use `gym.make` as usual):\n",
        "for env_name, env_class in _envs.items():\n",
        "    env_id = f\"brax_{env_name}-v0\"\n",
        "    entry_point = partial(create_gym_env, env_name=env_name)\n",
        "    if env_id not in gym.envs.registry.env_specs:\n",
        "        print(f\"Registring brax's '{env_name}' env under id '{env_id}'.\")\n",
        "        gym.register(env_id, entry_point=entry_point)\n",
        "\n",
        "# Simple utility function for benchmarking below.\n",
        "def tick(name: str = \"\"):\n",
        "    global _times\n",
        "    _times.append(time.time())\n",
        "    elapsed = _times[-1] - _times[-2]\n",
        "    if name:\n",
        "        print(f\"Time for {name}: {elapsed}\", flush=True)\n",
        "    return elapsed\n",
        "\n",
        "CUDA = torch.cuda.is_available()\n",
        "if CUDA:\n",
        "    # BUG: (@lebrice): Getting a weird \"CUDA error: out of memory\" RuntimeError\n",
        "    # during JIT, which can be \"fixed\" by first creating a dummy cuda tensor!\n",
        "    v = torch.ones(1, device=\"cuda\")\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Registring brax's 'fetch' env under id 'brax_fetch-v0'.\n",
            "Registring brax's 'ant' env under id 'brax_ant-v0'.\n",
            "Registring brax's 'grasp' env under id 'brax_grasp-v0'.\n",
            "Registring brax's 'halfcheetah' env under id 'brax_halfcheetah-v0'.\n",
            "Registring brax's 'humanoid' env under id 'brax_humanoid-v0'.\n",
            "Registring brax's 'ur5e' env under id 'brax_ur5e-v0'.\n",
            "Registring brax's 'reacher' env under id 'brax_reacher-v0'.\n",
            "Registring brax's 'reacherangle' env under id 'brax_reacherangle-v0'.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yP-4jmL8losB"
      },
      "source": [
        "## Benchmarking the performance of the Pytorch'ed Brax gym Env:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9mqL2S67losC",
        "outputId": "5d8b05e8-de77-4794-84c8-3813b181bd53"
      },
      "source": [
        "_times = [time.time()]\n",
        "\n",
        "# Number of parallel environments\n",
        "batch_size = 2048  #@param { type:\"slider\", min:0, max:4096, step: 1 }\n",
        "\n",
        "# Number of steps to take in the batched env.\n",
        "n_steps = 1000\n",
        "\n",
        "env = gym.make(\"brax_halfcheetah-v0\", batch_size=batch_size)\n",
        "tick(\"creating the env\")\n",
        "\n",
        "env = JaxToTorchWrapper(env)\n",
        "tick(\"wrapping the env\")\n",
        "\n",
        "obs = env.reset()  # this can be relatively slow (~10 secs)\n",
        "tick(\"first reset\") \n",
        "\n",
        "obs, reward, done, info = env.step(env.action_space.sample())\n",
        "tick(\"first step\")  # this can be relatively slow (~10 secs)\n",
        "\n",
        "obs, reward, done, info = env.step(env.action_space.sample())\n",
        "tick(\"second step\")  # this can be relatively slow (~10 secs)\n",
        "\n",
        "_times.clear()\n",
        "_times.append(time.time())\n",
        "\n",
        "# create a progress bar that displays step rate\n",
        "pbar = tqdm.tqdm(range(n_steps), unit_scale=batch_size)\n",
        "for i in pbar:\n",
        "    # for GPU, we create the pytorch data directly on device in order to avoid\n",
        "    # expensive cross-device copying\n",
        "    if CUDA:\n",
        "        action = torch.rand(env.action_space.shape, device=\"cuda\") * 2 - 1\n",
        "    else:\n",
        "        action = env.action_space.sample()\n",
        "    obs, rewards, done, info = env.step(action)\n",
        "    tick()\n",
        "\n",
        "\n",
        "elapsed_times = [_times[i+1] - _times[i] for i in range(len(_times)-1)]\n",
        "\n",
        "time_per_step_avg = np.mean(elapsed_times)\n",
        "time_per_step_std = np.std(elapsed_times)\n",
        "\n",
        "frequency =  1 / time_per_step_avg\n",
        "effective_frequency = (batch_size or 1) * frequency\n",
        "if (isinstance(obs, torch.Tensor)):\n",
        "  device = obs.device\n",
        "else:\n",
        "  device = obs.device_buffer.device()\n",
        "\n",
        "print(f\"\\nDevice used: {device}\")\n",
        "print(f\"Number of parallel environments: {batch_size}\")\n",
        "print(f\"Average time per batched step:  {time_per_step_avg} ± {time_per_step_std} seconds\")\n",
        "print(f\"Frequency (after first two steps): ~{frequency:.3f} batched steps / second.\")\n",
        "print(f\"Effective Frequency (after first two steps): ~{effective_frequency:.3f} steps / second.\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time for creating the env: 7.492677688598633\n",
            "Time for wrapping the env: 0.0012292861938476562\n",
            "Time for first reset: 8.5595543384552\n",
            "Time for first step: 4.626008033752441\n",
            "Time for second step: 4.190889835357666\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 2048000/2048000 [00:05<00:00, 364426.28it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Device used: cuda:0\n",
            "Number of parallel environments: 2048\n",
            "Average time per batched step:  0.005624113082885743 ± 0.0010372177569917425 seconds\n",
            "Frequency (after first two steps): ~177.806 batched steps / second.\n",
            "Effective Frequency (after first two steps): ~364146.305 steps / second.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}