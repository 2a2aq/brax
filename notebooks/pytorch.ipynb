{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "orig_nbformat": 4,
    "language_info": {
      "name": "python",
      "version": "3.8.10",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.10 64-bit ('brax': conda)"
    },
    "interpreter": {
      "hash": "1812c2cdf0067f1c111fe8b907e8717aab013f923026de8fa0a048a0a07a7c66"
    },
    "colab": {
      "name": "Brax Pytorch.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqqSTrs_lor_"
      },
      "source": [
        "# Using Brax with PyTorch\n",
        "\n",
        "Brax integrates seamlessly with [PyTorch](https://pytorch.org/) via a [Gym interface](https://github.com/openai/gym) without sacrificing any performance.  In this notebook we demonstrate how to operate Brax by passing in PyTorch arrays.\n",
        "\n",
        "Author: @lebrice (Fabrice Normandin)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z4sNbJ4smdIh"
      },
      "source": [
        "#@title Colab setup and imports\n",
        "#@markdown ### ⚠️ PLEASE NOTE:\n",
        "#@markdown Brax and PyTorch can share GPUs but not TPUs.  To access a GPU runtime: from the Colab menu choose Runtime > Change Runtime Type, then select **'GPU'** in the dropdown.\n",
        "#@markdown\n",
        "#@markdown Using TPU is OK too, but then PyTorch should run on CPU.\n",
        "\n",
        "import os\n",
        "import time\n",
        "from functools import partial\n",
        "\n",
        "import gym\n",
        "import numpy as np\n",
        "import torch\n",
        "import tqdm\n",
        "\n",
        "try:\n",
        "  import brax\n",
        "except ImportError:\n",
        "  from IPython.display import clear_output\n",
        "  !pip install git+https://github.com/google/brax.git@main\n",
        "  clear_output()\n",
        "\n",
        "from brax.envs.to_torch import JaxToTorchWrapper\n",
        "from brax.envs import _envs, create_gym_env\n",
        "\n",
        "if 'COLAB_TPU_ADDR' in os.environ:\n",
        "  from jax.tools import colab_tpu\n",
        "  colab_tpu.setup_tpu()\n",
        "\n",
        "CUDA_AVAILABLE = torch.cuda.is_available()\n",
        "if CUDA_AVAILABLE:\n",
        "    # BUG: (@lebrice): Getting a weird \"CUDA error: out of memory\" RuntimeError\n",
        "    # during JIT, which can be \"fixed\" by first creating a dummy cuda tensor!\n",
        "    v = torch.ones(1, device=\"cuda\")"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gKby-SR2losA",
        "outputId": "3c15da42-0d7e-48c6-8a56-0419af13cad7"
      },
      "source": [
        "#@title Registering Brax environments in Gym\n",
        "\n",
        "for env_name, env_class in _envs.items():\n",
        "    env_id = f\"brax_{env_name}-v0\"\n",
        "    entry_point = partial(create_gym_env, env_name=env_name)\n",
        "    if env_id not in gym.envs.registry.env_specs:\n",
        "        print(f\"Registring brax's '{env_name}' env under id '{env_id}'.\")\n",
        "        gym.register(env_id, entry_point=entry_point)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Registring brax's 'fetch' env under id 'brax_fetch-v0'.\n",
            "Registring brax's 'ant' env under id 'brax_ant-v0'.\n",
            "Registring brax's 'grasp' env under id 'brax_grasp-v0'.\n",
            "Registring brax's 'halfcheetah' env under id 'brax_halfcheetah-v0'.\n",
            "Registring brax's 'humanoid' env under id 'brax_humanoid-v0'.\n",
            "Registring brax's 'ur5e' env under id 'brax_ur5e-v0'.\n",
            "Registring brax's 'reacher' env under id 'brax_reacher-v0'.\n",
            "Registring brax's 'reacherangle' env under id 'brax_reacherangle-v0'.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9mqL2S67losC",
        "outputId": "d517bf15-a5ce-4191-bd75-87655a1b8a32"
      },
      "source": [
        "#@title Benchmarking steps per second of Pytorch -> Gym -> Brax\n",
        "\n",
        "def tick(name: str = \"\"):\n",
        "    global _times\n",
        "    _times.append(time.time())\n",
        "    elapsed = _times[-1] - _times[-2]\n",
        "    if name:\n",
        "        print(f\"Time for {name}: {elapsed}\", flush=True)\n",
        "    return elapsed\n",
        "\n",
        "_times = [time.time()]\n",
        "\n",
        "# Number of parallel environments\n",
        "batch_size = 2048  #@param { type:\"slider\", min:0, max:4096, step: 1 }\n",
        "\n",
        "# Number of steps to take in the batched env.\n",
        "n_steps = 1000\n",
        "\n",
        "env = gym.make(\"brax_halfcheetah-v0\", batch_size=batch_size)\n",
        "tick(\"creating the env\")\n",
        "\n",
        "env = JaxToTorchWrapper(env)\n",
        "tick(\"wrapping the env\")\n",
        "\n",
        "obs = env.reset()  # this can be relatively slow (~10 secs)\n",
        "tick(\"first reset\") \n",
        "\n",
        "obs, reward, done, info = env.step(env.action_space.sample())\n",
        "tick(\"first step\")  # this can be relatively slow (~10 secs)\n",
        "\n",
        "obs, reward, done, info = env.step(env.action_space.sample())\n",
        "tick(\"second step\")  # this can be relatively slow (~10 secs)\n",
        "\n",
        "_times.clear()\n",
        "_times.append(time.time())\n",
        "\n",
        "# create a progress bar that displays step rate\n",
        "pbar = tqdm.tqdm(range(n_steps), unit_scale=batch_size)\n",
        "for i in pbar:\n",
        "    # for GPU, we create the pytorch data directly on device in order to avoid\n",
        "    # expensive cross-device copying\n",
        "    if CUDA_AVAILABLE:\n",
        "        action = torch.rand(env.action_space.shape, device=\"cuda\") * 2 - 1\n",
        "    else:\n",
        "        action = env.action_space.sample()\n",
        "    obs, rewards, done, info = env.step(action)\n",
        "    tick()\n",
        "\n",
        "elapsed_times = [_times[i+1] - _times[i] for i in range(len(_times)-1)]\n",
        "\n",
        "time_per_step_avg = np.mean(elapsed_times)\n",
        "time_per_step_std = np.std(elapsed_times)\n",
        "\n",
        "frequency =  1 / time_per_step_avg\n",
        "effective_frequency = (batch_size or 1) * frequency\n",
        "if (isinstance(obs, torch.Tensor)):\n",
        "  device = obs.device\n",
        "else:\n",
        "  device = obs.device_buffer.device()\n",
        "\n",
        "print(f\"\\nDevice used: {device}\")\n",
        "print(f\"Number of parallel environments: {batch_size}\")\n",
        "print(f\"Average time per batched step:  {time_per_step_avg} ± {time_per_step_std} seconds\")\n",
        "print(f\"Frequency (after first two steps): ~{frequency:.3f} batched steps / second.\")\n",
        "print(f\"Effective Frequency (after first two steps): ~{effective_frequency:.3f} steps / second.\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time for creating the env: 22.936784505844116\n",
            "Time for wrapping the env: 0.0013244152069091797\n",
            "Time for first reset: 8.07045602798462\n",
            "Time for first step: 4.180543899536133\n",
            "Time for second step: 3.7199959754943848\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 2048000/2048000 [00:05<00:00, 397047.16it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Device used: cuda:0\n",
            "Number of parallel environments: 2048\n",
            "Average time per batched step:  0.005167229413986206 ± 0.0018089269095354766 seconds\n",
            "Frequency (after first two steps): ~193.527 batched steps / second.\n",
            "Effective Frequency (after first two steps): ~396343.927 steps / second.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}